{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b9048b",
   "metadata": {},
   "source": [
    "# Generate the model from processed.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2806ba",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25153531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import clone\n",
    "from sklearn.base import RegressorMixin, TransformerMixin, BaseEstimator\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import r_regression, SelectPercentile\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a5385",
   "metadata": {},
   "source": [
    "Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a6e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   difficulty  average_tempo  average_bpm  note_count  tick_count  \\\n",
      "0           4  413197.921548   145.208862        3656      155036   \n",
      "1           5  600000.000000   100.000000        3926       28352   \n",
      "2           5  600000.000000   100.000000        3926       28352   \n",
      "3           5  350818.280820   171.028716        2674      151680   \n",
      "4           5  441740.064844   135.826484        3570       22816   \n",
      "\n",
      "   note_density  tempo_deviation  unique_note_count  total_duration  \\\n",
      "0      0.023582     97297.985266                 65          155036   \n",
      "1      0.138473         0.000000                 57           28352   \n",
      "2      0.138473         0.000000                 57           28352   \n",
      "3      0.017629     11112.029407                 73          452520   \n",
      "4      0.156469      7352.818211                 63           43392   \n",
      "\n",
      "   overlapping_notes  ...  odd_time_signature_count  consecutive_note_std  \\\n",
      "0                283  ...                         1             43.313047   \n",
      "1                227  ...                         0              7.385267   \n",
      "2                227  ...                         0              7.385267   \n",
      "3               1347  ...                         0             70.806104   \n",
      "4               1541  ...                         0             15.330321   \n",
      "\n",
      "   pitch_range  average_polyphony  tempo_change_count  max_polyphony  \\\n",
      "0         67.0           1.084545                  20              4   \n",
      "1         65.0           1.061368                   1              3   \n",
      "2         65.0           1.061368                   1              3   \n",
      "3         77.0           2.149518                 134              6   \n",
      "4         71.0           1.767327                  71              4   \n",
      "\n",
      "   note_to_note_transition  note_to_chord_transition  \\\n",
      "0                  41703.0               2585.333333   \n",
      "1                  32831.0                660.500000   \n",
      "2                  32831.0                660.500000   \n",
      "3                   5601.0               1266.666667   \n",
      "4                    253.0               3325.166667   \n",
      "\n",
      "   chord_to_note_transition  chord_to_chord_transition  \n",
      "0               2913.833333                 405.333333  \n",
      "1                700.000000                  19.500000  \n",
      "2                700.000000                  19.500000  \n",
      "3               1291.666667                   0.000000  \n",
      "4               3297.833333                3688.833333  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/processed.csv')\n",
    "\n",
    "# makes models perform worse\n",
    "# df['difficulty_transform'], lambda_value = stats.boxcox(df['difficulty'])\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff93a542",
   "metadata": {},
   "source": [
    "## Selecting best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bc7fc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features kept: Index(['note_count', 'note_density', 'unique_note_count', 'notes_per_second',\n",
      "       'pitch_range', 'tempo_change_count', 'note_to_note_transition',\n",
      "       'note_to_chord_transition', 'chord_to_note_transition',\n",
      "       'chord_to_chord_transition'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['difficulty'])\n",
    "y = df['difficulty']\n",
    "\n",
    "features_names = X.columns\n",
    "\n",
    "# Keep the best features\n",
    "selector = SelectPercentile(r_regression, percentile=40)\n",
    "X = selector.fit_transform(X, y)\n",
    "features_kept = features_names[selector.get_support()]\n",
    "print(\"Features kept:\", features_kept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ddaef4",
   "metadata": {},
   "source": [
    "## Scaling data and splitting\n",
    "features have a large difference in scale, scaling makes model perform slightly better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bbaa978",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Removes randomness from the model across runs\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=rng)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae250bd6",
   "metadata": {},
   "source": [
    "## Tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de0e7942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuning hyperparamerts for  XGB\n",
      "Best parameters for XGB: {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 2, 'min_child_weight': 2, 'n_estimators': 1000}\n",
      "Best cross-validation score for XGB: 0.6936408629359881\n",
      "tuning hyperparamerts for  RandomForest\n",
      "Best parameters for RandomForest: {'max_depth': 10, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best cross-validation score for RandomForest: 0.7179281339930079\n",
      "tuning hyperparamerts for  SVR\n",
      "Best parameters for SVR: {'C': 1, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Best cross-validation score for SVR: 0.669228374002386\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "  'XGB': {\n",
    "    'model': XGBRegressor(random_state=rng),\n",
    "    'params': {\n",
    "        'n_estimators': [100, 200, 500, 1000],\n",
    "        'gamma': [0.005, 0.01, 0.1, 0],\n",
    "        'max_depth': [1, 2, 3, 6, 9],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.015, 1],\n",
    "        'min_child_weight': [1, 2, 3],\n",
    "    }\n",
    "  },\n",
    "  'RandomForest': {\n",
    "    'model': RandomForestRegressor(random_state=rng),\n",
    "    'params': {\n",
    "      'n_estimators': [100, 200, 500],\n",
    "      'max_features': ['sqrt', 'log2', None],\n",
    "      'min_samples_split': [2, 5, 10],\n",
    "      'max_depth': [None, 1, 2, 10, 20, 30],\n",
    "      'max_leaf_nodes': [None, 2, 5, 10],\n",
    "    }\n",
    "  },\n",
    "  'SVR': {\n",
    "    'model': SVR(),\n",
    "    'params': {\n",
    "      'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "      'C': [0.1, 1, 10, 20, 50, 100],\n",
    "      'gamma': ['scale', 'auto'],\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "results =  {}\n",
    "\n",
    "for name, config in model_params.items():\n",
    "  print('tuning hyperparamerts for ', name)\n",
    "  # Can edit cv to be more or less, less is faster\n",
    "  grid_search = GridSearchCV(config['model'], config['params'], cv=3, scoring='r2')\n",
    "  grid_search.fit(X_train, y_train)\n",
    "  \n",
    "  results[name] = {\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'best_score': grid_search.best_score_,\n",
    "  }\n",
    "  \n",
    "  print(f\"Best parameters for {name}:\", grid_search.best_params_)\n",
    "  print(f\"Best cross-validation score for {name}:\", grid_search.best_score_)\n",
    "\n",
    "with open(f'model_params.json', 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac170457",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('model_params.json', 'r') as f:\n",
    "    params = json.load(f)\n",
    "    \n",
    "models = {\n",
    "    'XGB': XGBRegressor(**params['XGB']['best_params']),\n",
    "    'RandomForest': RandomForestRegressor(**params['RandomForest']['best_params']),\n",
    "    'SVR': SVR(**params['SVR']['best_params'])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe349e",
   "metadata": {},
   "source": [
    "Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28b40da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "def rmse_cross_validation(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c04c3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGB\n",
      "Mean squared error: 0.2417248541571918\n",
      "Mean absolute error: 0.36344913549201435\n",
      "R^2 score: 0.8882067895606184\n",
      "Root mean squared error: 0.7845504811494957\n",
      "======================\n",
      "Model: RandomForest\n",
      "Mean squared error: 0.3185553868269104\n",
      "Mean absolute error: 0.42526585360743696\n",
      "R^2 score: 0.8526741094940077\n",
      "Root mean squared error: 0.717761674469414\n",
      "======================\n",
      "Model: SVR\n",
      "Mean squared error: 0.23036365208346002\n",
      "Mean absolute error: 0.3998919642275555\n",
      "R^2 score: 0.8934611323906159\n",
      "Root mean squared error: 0.768929666256393\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Model: {name}\")\n",
    "    # Round and clip to ensure valid difficulty levels\n",
    "    y_pred_rounded = np.clip(np.round(y_pred), 1, 5)\n",
    "    print(\"Mean squared error:\", mean_squared_error(y_test, y_pred))\n",
    "    print(\"Mean absolute error:\", mean_absolute_error(y_test, y_pred))\n",
    "    print(\"R^2 score:\", r2_score(y_test, y_pred))\n",
    "    print(\"Root mean squared error:\", rmse_cross_validation(model).mean())\n",
    "    print(\"======================\")\n",
    "    # Save the model to disk\n",
    "    with open(f'./models/{name}_saved_model.pkl', 'wb') as file:\n",
    "      pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6bbe0",
   "metadata": {},
   "source": [
    "## AveragingModel using the models trained earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e745bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# referenced from https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models, scaler):\n",
    "        self.models = models\n",
    "        self.scaler = scaler\n",
    "\n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3a1dd",
   "metadata": {},
   "source": [
    "Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7c7eb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Averaged base models score: 0.7145 (0.1652)\n",
      "\n",
      "Mean squared error: 0.21052522639447882\n",
      "Mean absolute error: 0.35158797977946504\n",
      "R^2 score: 0.9026360321152097\n"
     ]
    }
   ],
   "source": [
    "averaged_models = AveragingModels(models=list(models.values()), scaler = scaler)\n",
    "score = rmse_cross_validation(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "\n",
    "averaged_models.fit(X_train, y_train)\n",
    "y_pred = averaged_models.predict(X_test)\n",
    "print(\"Mean squared error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"Mean absolute error:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"R^2 score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "with open(f'./models/averaged_models.pkl', 'wb') as file:\n",
    "    pickle.dump(averaged_models, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac18d1",
   "metadata": {},
   "source": [
    "# Predicting the difficulty for the rest of the songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620bddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanbi\\OneDrive\\Desktop\\projects\\music-ai\\test\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2732: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('./models/averaged_models.pkl', 'rb') as file:\n",
    "    averaged_models = pickle.load(file)\n",
    "\n",
    "df = pd.read_csv('data/all_song_features.csv')\n",
    "\n",
    "# Select only the columns used during training\n",
    "X_unseen = df.loc[:, features_kept]\n",
    "\n",
    "# Scale the data using the same scaler from training\n",
    "X_unseen = scaler.transform(X_unseen)\n",
    "\n",
    "y_pred = averaged_models.predict(X_unseen)\n",
    "\n",
    "predictions = pd.DataFrame({\n",
    "    'file': df['file'],\n",
    "    'predicted_difficulty': y_pred\n",
    "}).to_csv('predictions.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
